<!DOCTYPE html>
<html lang="en-ca">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.74.1" />


<title>Predictive model for the Oscars - Blog—Max Turgeon</title>
<meta property="og:title" content="Predictive model for the Oscars - Blog—Max Turgeon">


  <link href='/icon.png' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/avatar.jpeg"
         width="50"
         height="50"
         alt="Max Turgeon">
  </a>

  <ul class="nav-links">
    
    <li><a href="/">Home</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">11 min read</span>
    

    <h1 class="article-title">Predictive model for the Oscars</h1>

    
    <span class="article-date">2018-02-25</span>
    

    <div class="article-content">
      <p>A few years ago, as part of the graduate course <em>Data Analysis and Report Writing</em> in the Department of Epidemiology, Biostatistics and Occupational Health at McGill University, we explored the topic of predictive modeling using a dataset containing movies, directors and actors who were nominated for an Academy Award. The goal was to select some variables and build a predictive model for the winner in four categories: Best Picture, Best Director, Best Actor, and Best Actress. As a movie fan, this was the dream assignment: I could combine my love of movies with my love of statistics! And it payed off: I was the only one in my class to correctly predict all four winners.</p>
<p>Over the years, I&rsquo;ve continued using the model, but I&rsquo;ve also tweaked it a bit. In this post, I want to describe my model and present the predictions for this year (see also ({{ site.github.url }}/oscar-predictions-2018)).</p>
<h2 id="conditional-logistic-regression">Conditional logistic regression</h2>
<p>The modeling method I&rsquo;ve selected is <em>conditional logistic regression</em>. I will give more details about how it works, but at the main principle is as follows: <strong>we condition on the number of events (i.e. winners) per strata</strong>. As we will see, this principle leads to many desirable characteristics that make this an ideal tool for this situation:</p>
<ul>
<li>We get a set of predictions that add up to 1. In particular, we will never get two nominees with probabilities above 50%.</li>
<li>When all observations in a stratum have the same value for a given covariate, the contribution of this stratum to the estimation of the coefficient corresponding to this covariate simply drops from the likelihood. As a consequence, there is absolutely no problem in using the results from the Golden Globes (the first ceremony was in 1944) and those from the Screen Actors Guild Awards (the first ceremony was in 1995) in the same model.</li>
<li>We can model a different baseline probability (i.e. a different intercept) for each stratum, without using any degree of freedom.</li>
</ul>
<h3 id="a-little-bit-of-probability-calculus">A little bit of probability calculus</h3>
<p>Let \( Y_1,\ldots,Y_K \) be the winning indicator for each nominee in a given category on a given year. If we assume that only one candidate can win, i.e. if we condition on \( \sum Y_j = 1 \), the probability that the i-th nominee wins is
\[ P(Y_i = 1 \mid \sum Y_j = 1) = \frac{P(Y_i = 1)}{\sum P(Y_j = 1)}. \]
If we model the (unconditional) probabilities using a log link and a linear predictor, we get
\[ P(Y_i = 1 \mid \sum Y_j = 1) = \frac{\exp(\beta X_i)}{\sum \exp(\beta X_j)}. \]
We then get the likelihood by multiplying these individual contributions over nominees and over strata (i.e. individual ceremonies). From these two equations, we can derive the three properties above: first, we clearly have
\[ \sum P(Y_i = 1 \mid \sum Y_j = 1) = 1. \]
Second, if all nominees have the same value for a given predictor, then we can factor it from the numerator and the denominator, and hence cancel it out. Finally, the intercept is one such predictor that is common to all nominees, and so it cancels out. In particular, the intercept could be different for each stratum&ndash;it is actually non-identifiable, so we can&rsquo;t test if that is indeed the case.</p>
<h2 id="choosing-the-predictors">Choosing the predictors</h2>
<p>It is helpful to remember the following: we are <strong>not</strong> trying to describe the data-generating mechanism that leads to a movie winning an Oscar; we are simply trying to mimic it as closely as possible. The distinction is important: in the former case, there are simply too many variables in play and not enough data to estimate all important contributing factors; in the latter case, we can use the precursor awards. And conceptually, this also makes a lot of sense: the Academy is composed of roughly 7000 members who also vote for other industry awards during the season.</p>
<p>Keeping all this in mind, the predictors I used are the winners of several critics awards (Chicago Film Critics Association, Los Angeles Film Critics Association, National Board of Review, National Society of Film Critics, and the New York Film Critics Circle), several industry awards (British Academy of Film and Television Arts, Screen Actors Guild, Producers Guild of America, and Directors Guild of America), as well as the results from the Golden Globes. There is one exception: for Best Picture, I also use the number of Academy Award nominations.</p>
<p>Finally, we should also keep in mind the following rule-of-thumb: you should have around 5-10 events per predictor in order to get a stable model, i.e. control for over-fitting.</p>
<h2 id="collecting-the-data">Collecting the data</h2>
<p>Starting this year, I&rsquo;ve decided to scrape the web to collect the data. Of course, web scrapers are by definition non-robust, and therefore I may have to make major changes in the future. But I like the flexibility it provides me, since I can easily add other awards. But this also means I had to do <strong>a lot</strong> of data cleaning. For those of you who are interested, I encourage you to look at my Github repo: (github.com/turgeonmaxime/oscar-db).</p>
<h2 id="predicting-best-picture">Predicting Best Picture</h2>
<p>We are finally ready to make some predictions! I&rsquo;ve put together a dataset that contains all the information we need (again, look at my <a href="github.com/turgeonmaxime/oscar-db">Github repo</a> for the code):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">library</span>(tidyverse)
<span style="color:#a6e22e">library</span>(magrittr)

<span style="color:#a6e22e">names</span>(data_bestpic)
</code></pre></div><pre><code>##  [1] &quot;Year&quot;     &quot;Ceremony&quot; &quot;Award&quot;    &quot;Winner&quot;   &quot;Name&quot;     &quot;Film&quot;    
##  [7] &quot;num_nom&quot;  &quot;Gdr&quot;      &quot;Gmc&quot;      &quot;Bafta&quot;    &quot;PGA&quot;      &quot;DGA&quot;     
## [13] &quot;SGA&quot;      &quot;CFCA&quot;     &quot;LAFCA&quot;    &quot;NBR&quot;      &quot;NSFC&quot;     &quot;NYFCC&quot;
</code></pre><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">data_bestpic <span style="color:#f92672">%&gt;%</span> 
    <span style="color:#a6e22e">filter</span>(Year <span style="color:#f92672">==</span> <span style="color:#ae81ff">2016</span>)
</code></pre></div><pre><code>## # A tibble: 9 x 18
##    Year Ceremony        Award Winner                  Name
##   &lt;chr&gt;    &lt;dbl&gt;        &lt;chr&gt;  &lt;dbl&gt;                 &lt;chr&gt;
## 1  2016       89 Best Picture      1             Moonlight
## 2  2016       89 Best Picture      0               Arrival
## 3  2016       89 Best Picture      0                Fences
## 4  2016       89 Best Picture      0         Hacksaw Ridge
## 5  2016       89 Best Picture      0    Hell or High Water
## 6  2016       89 Best Picture      0        Hidden Figures
## 7  2016       89 Best Picture      0            La La Land
## 8  2016       89 Best Picture      0                  Lion
## 9  2016       89 Best Picture      0 Manchester by the Sea
## # ... with 13 more variables: Film &lt;chr&gt;, num_nom &lt;int&gt;, Gdr &lt;dbl&gt;,
## #   Gmc &lt;dbl&gt;, Bafta &lt;dbl&gt;, PGA &lt;dbl&gt;, DGA &lt;dbl&gt;, SGA &lt;dbl&gt;, CFCA &lt;dbl&gt;,
## #   LAFCA &lt;dbl&gt;, NBR &lt;dbl&gt;, NSFC &lt;dbl&gt;, NYFCC &lt;dbl&gt;
</code></pre><p>We will use the <code>survival</code> package to fit a conditional logistic regression model:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">library</span>(survival)
pred_pic <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">c</span>(<span style="color:#e6db74">&#34;num_nom&#34;</span>, <span style="color:#e6db74">&#34;Gdr&#34;</span>, <span style="color:#e6db74">&#34;Gmc&#34;</span>, <span style="color:#e6db74">&#34;Bafta&#34;</span>, <span style="color:#e6db74">&#34;PGA&#34;</span>, <span style="color:#e6db74">&#34;DGA&#34;</span>,
              <span style="color:#e6db74">&#34;SGA&#34;</span>, <span style="color:#e6db74">&#34;CFCA&#34;</span>, <span style="color:#e6db74">&#34;LAFCA&#34;</span>, <span style="color:#e6db74">&#34;NBR&#34;</span>, <span style="color:#e6db74">&#34;NSFC&#34;</span>, <span style="color:#e6db74">&#34;NYFCC&#34;</span>)

fit_pic <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">clogit</span>(<span style="color:#a6e22e">formula</span>(<span style="color:#a6e22e">paste</span>(<span style="color:#e6db74">&#34;Winner ~&#34;</span>, <span style="color:#a6e22e">paste</span>(pred_pic, collapse <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;+&#34;</span>), 
                                <span style="color:#e6db74">&#34;+ strata(Year)&#34;</span>)),
                  data <span style="color:#f92672">=</span> <span style="color:#a6e22e">filter</span>(data_bestpic, Winner <span style="color:#f92672">!=</span> <span style="color:#ae81ff">-1</span>))
</code></pre></div><p>All current nominees are identified using <code>Winner == -1</code>. We can look at the summary of this model:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#a6e22e">summary</span>(fit_pic)
</code></pre></div><pre><code>## Call:
## coxph(formula = Surv(rep(1, 435L), Winner) ~ num_nom + Gdr + 
##     Gmc + Bafta + PGA + DGA + SGA + CFCA + LAFCA + NBR + NSFC + 
##     NYFCC + strata(Year), data = filter(data_bestpic, Winner != 
##     -1), method = &quot;exact&quot;)
## 
##   n= 435, number of events= 78 
## 
##             coef exp(coef) se(coef)      z Pr(&gt;|z|)    
## num_nom  0.39531   1.48484  0.09736  4.060 4.90e-05 ***
## Gdr      1.51457   4.54745  0.56314  2.689  0.00716 ** 
## Gmc      0.58223   1.79003  0.70220  0.829  0.40702    
## Bafta    0.25220   1.28685  0.68237  0.370  0.71169    
## PGA     -0.17996   0.83530  0.76309 -0.236  0.81356    
## DGA      1.98418   7.27306  0.39928  4.969 6.72e-07 ***
## SGA      2.17629   8.81352  0.81292  2.677  0.00743 ** 
## CFCA     1.56838   4.79888  0.77942  2.012  0.04419 *  
## LAFCA   -0.80066   0.44903  0.72926 -1.098  0.27225    
## NBR      0.32401   1.38266  0.50704  0.639  0.52281    
## NSFC     2.24504   9.44077  0.75837  2.960  0.00307 ** 
## NYFCC    0.28481   1.32951  0.41083  0.693  0.48815    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
##         exp(coef) exp(-coef) lower .95 upper .95
## num_nom    1.4848     0.6735    1.2269     1.797
## Gdr        4.5474     0.2199    1.5081    13.712
## Gmc        1.7900     0.5586    0.4520     7.089
## Bafta      1.2868     0.7771    0.3378     4.902
## PGA        0.8353     1.1972    0.1872     3.727
## DGA        7.2731     0.1375    3.3255    15.907
## SGA        8.8135     0.1135    1.7914    43.362
## CFCA       4.7989     0.2084    1.0416    22.110
## LAFCA      0.4490     2.2270    0.1075     1.875
## NBR        1.3827     0.7232    0.5118     3.735
## NSFC       9.4408     0.1059    2.1354    41.738
## NYFCC      1.3295     0.7522    0.5943     2.974
## 
## Rsquare= 0.294   (max possible= 0.454 )
## Likelihood ratio test= 151.5  on 12 df,   p=0
## Wald test            = 56.58  on 12 df,   p=9.413e-08
## Score (logrank) test = 170.7  on 12 df,   p=0
</code></pre><p>(<strong>Note</strong>: we can actually show that the conditional logistic likelihood has a similar form to a Cox regression partial likelihood. The <code>survival</code> package uses this trick to fit the likelihood, and this is why we get a call to <code>coxph</code>.)</p>
<p>As we can see from the summary, the largest odds ratios are those for the Screen Actors Guild Awards (<em>Best Performance by an Ensemble Cast</em>) and the National Society of Film Critics. But we can also see that some of these confidence intervals are <em>huge</em>! This is clearly a consequence of the fact that we only have 78 events&hellip;</p>
<p>Now, let&rsquo;s make predictions for this year:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#75715e"># Extract coefficients</span>
coef_pic <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">summary</span>(fit_pic)<span style="color:#f92672">$</span>coefficients[,<span style="color:#ae81ff">1</span>]
<span style="color:#75715e"># Look at this year&#39;s data</span>
current_pic <span style="color:#f92672">&lt;-</span> data_bestpic <span style="color:#f92672">%&gt;%</span> 
    <span style="color:#a6e22e">filter</span>(Winner <span style="color:#f92672">==</span> <span style="color:#ae81ff">-1</span>)
<span style="color:#75715e"># We get the predictions using a bit of matrix multiplication</span>
exp_pic <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">exp</span>(<span style="color:#a6e22e">as.matrix</span>(current_pic[, <span style="color:#a6e22e">colnames</span>(current_pic) <span style="color:#f92672">%in%</span> pred_pic]) <span style="color:#f92672">%*%</span> coef_pic)
predict_pic <span style="color:#f92672">&lt;-</span> exp_pic<span style="color:#f92672">/</span><span style="color:#a6e22e">sum</span>(exp_pic)
<span style="color:#75715e"># Add the movie names</span>
<span style="color:#a6e22e">rownames</span>(predict_pic) <span style="color:#f92672">&lt;-</span> current_pic<span style="color:#f92672">$</span>Name
predict_pic
</code></pre></div><pre><code>##                                                  [,1]
## Call Me by Your Name                      0.001118943
## Darkest Hour                              0.003700077
## Dunkirk                                   0.003700077
## Get Out                                   0.002491896
## Lady Bird                                 0.398941783
## Phantom Thread                            0.005494039
## The Post                                  0.001562732
## The Shape of Water                        0.162246700
## Three Billboards Outside Ebbing, Missouri 0.420743752
</code></pre><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#75715e"># For more clarity</span>
<span style="color:#a6e22e">round</span>(<span style="color:#ae81ff">100</span> <span style="color:#f92672">*</span> predict_pic)
</code></pre></div><pre><code>##                                           [,1]
## Call Me by Your Name                         0
## Darkest Hour                                 0
## Dunkirk                                      0
## Get Out                                      0
## Lady Bird                                   40
## Phantom Thread                               1
## The Post                                     0
## The Shape of Water                          16
## Three Billboards Outside Ebbing, Missouri   42
</code></pre><p>If you follow the Oscar race, this may seems a bit surprising: as of February 25th, <em>The Shape of Water</em> is considered a front-runner, and yet the model only assigned about 16% chance of winning. Looking at the estimates of the regression coefficients, it is obvious why: <em>Three Billboards Outside Ebbing, Missouri</em> won the SAG award, <em>Lady Bird</em> won most of the critics awards, but <em>The Shape of Water</em> only won the DGA award and the PGA award. And the latter actually has a <em>negative</em> coefficient, meaning that a win at the PGA actually <em>decreases</em> the odds of winning an oscar (all other things held equal)&hellip;</p>
<p>Let&rsquo;s try to get a sense of the variability in these predictions. For this, we will use <em>parametric bootstrap</em>: since the conditional logistic likelihood satisfies the usual properties, the estimator for the regression coefficients has an approximate multivariate normal distribution. Since we also estimated its covariance matrix, we can actually sample for the distribution, get new point estimates of the coefficients, and recompute the winning probabilities.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">n.boot <span style="color:#f92672">&lt;-</span> <span style="color:#ae81ff">10000</span>
cov_pic <span style="color:#f92672">&lt;-</span> fit_pic<span style="color:#f92672">$</span>var
<span style="color:#75715e"># Sample from the estimator distribution</span>
samples_pic <span style="color:#f92672">&lt;-</span> MASS<span style="color:#f92672">::</span><span style="color:#a6e22e">mvrnorm</span>(n <span style="color:#f92672">=</span> n.boot, mu <span style="color:#f92672">=</span> coef_pic, Sigma <span style="color:#f92672">=</span> cov_pic)
<span style="color:#75715e"># Recompute the winning probabilities</span>
boot_prob_pic <span style="color:#f92672">&lt;-</span> purrr<span style="color:#f92672">::</span><span style="color:#a6e22e">map_df</span>(<span style="color:#a6e22e">seq_len</span>(n.boot), <span style="color:#a6e22e">function</span>(row){
    pred_log <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">as.matrix</span>(current_pic[, <span style="color:#a6e22e">colnames</span>(current_pic) <span style="color:#f92672">%in%</span> pred_pic]) <span style="color:#f92672">%*%</span> samples_pic[row,] <span style="color:#f92672">%&gt;%</span> 
        exp <span style="color:#f92672">%&gt;%</span> 
        t
    <span style="color:#a6e22e">colnames</span>(pred_log) <span style="color:#f92672">&lt;-</span> current_pic<span style="color:#f92672">$</span>Name
    pred <span style="color:#f92672">&lt;-</span> pred_log<span style="color:#f92672">/</span><span style="color:#a6e22e">sum</span>(pred_log)
    <span style="color:#a6e22e">mode</span>(pred) <span style="color:#f92672">&lt;-</span> <span style="color:#e6db74">&#34;list&#34;</span>
    <span style="color:#a6e22e">return</span>(<span style="color:#a6e22e">as.data.frame</span>(pred))
}) <span style="color:#f92672">%&gt;%</span> <span style="color:#a6e22e">mutate_all</span>(as.numeric)
</code></pre></div><p>Each row in <code>boot_prob_pic</code> corresponds to a new point estimate of the winning probabilities (I had to do some munging to make sure I get the right <code>data.frame</code>). From this we can look at the distribution of winning probabilities:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">boot_prob_pic <span style="color:#f92672">%&gt;%</span> 
    <span style="color:#a6e22e">sample_n</span>(<span style="color:#ae81ff">1000</span>) <span style="color:#f92672">%&gt;%</span> <span style="color:#75715e"># Take a random sample</span>
    <span style="color:#a6e22e">rename</span>(`The Shape of\nWater` <span style="color:#f92672">=</span> `The Shape of Water`,
           `Three Billboards\nOutside Ebbing, Missouri` <span style="color:#f92672">=</span> `Three Billboards Outside Ebbing, Missouri`) <span style="color:#f92672">%&gt;%</span> <span style="color:#75715e"># Add line breaks for plotting</span>
    <span style="color:#a6e22e">gather</span>(Movie, pred) <span style="color:#f92672">%&gt;%</span> 
    <span style="color:#a6e22e">ggplot</span>(<span style="color:#a6e22e">aes</span>(y <span style="color:#f92672">=</span> pred, x <span style="color:#f92672">=</span> Movie)) <span style="color:#f92672">+</span> <span style="color:#a6e22e">geom_boxplot</span>() <span style="color:#f92672">+</span> 
    <span style="color:#a6e22e">ylab</span>(<span style="color:#e6db74">&#34;Probability&#34;</span>) <span style="color:#f92672">+</span> <span style="color:#a6e22e">expand_limits</span>(y <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>) <span style="color:#f92672">+</span>
    <span style="color:#a6e22e">scale_y_continuous</span>(lim <span style="color:#f92672">=</span> <span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>), expand <span style="color:#f92672">=</span> <span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>)) <span style="color:#f92672">+</span>
    <span style="color:#a6e22e">theme</span>(axis.text.x <span style="color:#f92672">=</span> <span style="color:#a6e22e">element_text</span>(angle <span style="color:#f92672">=</span> <span style="color:#ae81ff">45</span>, hjust <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>))
</code></pre></div><p><img src="/figure/source/2018-02-25-oscar-prediction-model/unnamed-chunk-6-1.png" alt="plot of chunk unnamed-chunk-6"></p>
<p>As we can see, there is a lot of variation in these predictions, and therefore <em>The Shape of Water</em> winning is not incompatible with our model fit.</p>
<p>One more thing to keep in mind: people are generally bad with probabilities, and even worse with <em>conditional</em> probabilities. As a consequence, it may be the case that pundits are overestimating <em>The Shape of Water</em>&rsquo;s chances because they have trouble combining the information from all sources and giving them the proper weight. As an illustration of this, let&rsquo;s compare the conditional estimates (i.e. the ones we got from the model) to marginal estimates (i.e. that we would get by fitting only one covariate at a time):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r"><span style="color:#75715e"># Window function to get marginal coefficients</span>
marg_coef <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">function</span>(pred) {
    <span style="color:#a6e22e">sapply</span>(<span style="color:#a6e22e">seq_along</span>(pred), <span style="color:#a6e22e">function</span>(i)
        <span style="color:#a6e22e">clogit</span>(<span style="color:#a6e22e">formula</span>(<span style="color:#a6e22e">paste</span>(<span style="color:#e6db74">&#34;Winner ~&#34;</span>, pred[i], <span style="color:#e6db74">&#34;+ strata(Year)&#34;</span>)),
               data <span style="color:#f92672">=</span> <span style="color:#a6e22e">filter</span>(data_bestpic, Winner <span style="color:#f92672">!=</span> <span style="color:#ae81ff">-1</span>)) <span style="color:#f92672">%&gt;%</span> 
            coef
    )
    }

<span style="color:#75715e"># Order the predictors</span>
levels_pred <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">c</span>(<span style="color:#e6db74">&#34;num_nom&#34;</span>, <span style="color:#e6db74">&#34;CFCA&#34;</span>, <span style="color:#e6db74">&#34;LAFCA&#34;</span>, <span style="color:#e6db74">&#34;NBR&#34;</span>, <span style="color:#e6db74">&#34;NSFC&#34;</span>, <span style="color:#e6db74">&#34;NYFCC&#34;</span>, 
                 <span style="color:#e6db74">&#34;Gdr&#34;</span>, <span style="color:#e6db74">&#34;Gmc&#34;</span>, <span style="color:#e6db74">&#34;PGA&#34;</span>, <span style="color:#e6db74">&#34;DGA&#34;</span>, <span style="color:#e6db74">&#34;SGA&#34;</span>, <span style="color:#e6db74">&#34;Bafta&#34;</span>)

fit_pic <span style="color:#f92672">%&gt;%</span> 
    <span style="color:#a6e22e">coef</span>() <span style="color:#f92672">%&gt;%</span> 
    <span style="color:#a6e22e">data.frame</span>(Conditional <span style="color:#f92672">=</span> .) <span style="color:#f92672">%&gt;%</span> 
    <span style="color:#a6e22e">mutate</span>(Variable <span style="color:#f92672">=</span> <span style="color:#a6e22e">factor</span>(<span style="color:#a6e22e">row.names</span>(.), 
                             levels <span style="color:#f92672">=</span> levels_pred),
           Marginal <span style="color:#f92672">=</span> <span style="color:#a6e22e">marg_coef</span>(Variable),
           AwardType <span style="color:#f92672">=</span> <span style="color:#a6e22e">case_when</span>(
               Variable <span style="color:#f92672">%in%</span> <span style="color:#a6e22e">c</span>(<span style="color:#e6db74">&#34;CFCA&#34;</span>, <span style="color:#e6db74">&#34;LAFCA&#34;</span>, <span style="color:#e6db74">&#34;NBR&#34;</span>, <span style="color:#e6db74">&#34;NSFC&#34;</span>, <span style="color:#e6db74">&#34;NYFCC&#34;</span>) <span style="color:#f92672">~</span> <span style="color:#e6db74">&#34;Critics&#34;</span>,
               Variable <span style="color:#f92672">%in%</span> <span style="color:#a6e22e">c</span>(<span style="color:#e6db74">&#34;Gdr&#34;</span>, <span style="color:#e6db74">&#34;Gmc&#34;</span>, <span style="color:#e6db74">&#34;PGA&#34;</span>, <span style="color:#e6db74">&#34;DGA&#34;</span>, <span style="color:#e6db74">&#34;SGA&#34;</span>, <span style="color:#e6db74">&#34;Bafta&#34;</span>) <span style="color:#f92672">~</span> <span style="color:#e6db74">&#34;Industry&#34;</span>,
               <span style="color:#66d9ef">TRUE</span> <span style="color:#f92672">~</span> <span style="color:#e6db74">&#34;Other&#34;</span>
           )) <span style="color:#f92672">%&gt;%</span> 
    <span style="color:#a6e22e">gather</span>(Type, Estimate, Conditional, Marginal) <span style="color:#f92672">%&gt;%</span> 
    <span style="color:#a6e22e">ggplot</span>(<span style="color:#a6e22e">aes</span>(x <span style="color:#f92672">=</span> Variable, y <span style="color:#f92672">=</span> Estimate, fill <span style="color:#f92672">=</span> AwardType)) <span style="color:#f92672">+</span> 
    <span style="color:#a6e22e">geom_col</span>() <span style="color:#f92672">+</span> <span style="color:#a6e22e">facet_grid</span>(.~Type) <span style="color:#f92672">+</span>
    <span style="color:#a6e22e">theme</span>(axis.text.x <span style="color:#f92672">=</span> <span style="color:#a6e22e">element_text</span>(angle <span style="color:#f92672">=</span> <span style="color:#ae81ff">45</span>, hjust <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>))
</code></pre></div><pre><code>## Warning: attributes are not identical across measure variables;
## they will be dropped
</code></pre><p><img src="/figure/source/2018-02-25-oscar-prediction-model/unnamed-chunk-7-1.png" alt="plot of chunk unnamed-chunk-7"></p>
<p>As we can see, although on its own the PGA Award winner seems a good predictor (since 1989, the Academy Award and the PGA Award went to the same movie 19 times out of 28), once we include the information coming from the other guild awards it loses its predictive ability. Therefore, given that <em>The Shape of Water</em> only won the PGA and DGA awards, I believe <em>Three Billboards Outside Ebbing, Missouri</em> is the real front-runner.</p>
    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

